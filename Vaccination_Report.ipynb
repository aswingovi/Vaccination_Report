{"cells":[{"cell_type":"markdown","source":["#Hi Recruiter ,\nWelcome!\nI hope you and your family are doing excellent.\nHere is my solution"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7144ec7e-ee8c-487f-b2c9-9ffc777a06f8"}}},{"cell_type":"markdown","source":["#Problem Statement\n\n\nContext:\nI own a multi-specialty hospital chain with locations all across the world. My hospital is famous for vaccinations. Patients who come to my hospital (across the globe) will be given a user card with which they can access any of my hospitals in any location.\nCurrent Status:\nWe maintain customers data in Country wise database due to local policies. Now with legal approvals to build centralized data platform, we need our Data engineering team to collate data from individual databases into single source of truth having cleaned standardized data. The Architecture board has chosen Datalake and Databricks approach due to its capabilities with big data solutions. Business wants to generate a simple PowerBI report for top executives summarizing till date vaccination metrics. This report will be published and generated daily for the next 18 months. The 3 metrics mentioned below are required for the phase 1 release. \nDeliverables for assessment:\nSpark (Scala / PySpark) code that does the below\n\t•\tData cleansing / exception handling\n\t•\tData merging into single source of truth\n\t•\tData transformations and aggregations\n\t•\tCode should have unit testing\nMetrics needed:\n\t•\tTotal vaccination count by country and vaccination type\n\t•\t% vaccination in each country (You can assume values for total population)\n\t•\t% vaccination contribution by country (Sum of percentages add up to 100)\nExpected output format\n\t•\tMetric 1: CountryName, VaccinationType, No. of vaccinations\n\t•\tMetric 2: CountryName, % Vaccinated\n\t•\tMetric 3: CountryName, % Contribution"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"70b21b4e-7342-4290-b1d9-8e73f4d38753"}}},{"cell_type":"markdown","source":["Mounting the  the data lake storage gen2 containers"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a7a29499-8692-4da6-9bb0-476d8abf4322"}}},{"cell_type":"markdown","source":["Setting Up the Configs for mounting the container into databricks by providing client-id,directory-id and secret key"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"868f1288-cb97-4e24-b690-ca9b2257d06d"}}},{"cell_type":"code","source":["configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n           \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n           \"fs.azure.account.oauth2.client.id\": \"9bf25338-5be5-412c-9a2f-61a370d2493e\",\n           \"fs.azure.account.oauth2.client.secret\": \"Vnb8Q~EStumSfKTKW844VEiBbLdfI-qbjM.JWadR\",\n           \"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/cd74528c-db71-42b9-8f39-9f7ca2b78d05/oauth2/token\"}"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"839a4d3d-2aeb-4c40-a56e-0764aa16a4a3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Mounting the raw container where input files are placed"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2c66446e-c140-4293-b2d6-7d5a8e56f933"}}},{"cell_type":"code","source":["dbutils.fs.mount(\n  source = \"abfss://raw@aswin.dfs.core.windows.net/\",\n  mount_point = \"/mnt/raw\",\n  extra_configs = configs)\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bc6d7550-8a47-42b6-9bbc-733be4338f9e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">ExecutionError</span>                            Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-4499162618750922&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> dbutils.fs.mount(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">      2</span>   source <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">&#34;abfss://raw@aswin.dfs.core.windows.net/&#34;</span><span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span>   mount_point <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">&#34;/mnt/raw&#34;</span><span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span>   extra_configs = configs)\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> \n\n<span class=\"ansi-green-fg\">/databricks/python_shell/dbruntime/dbutils.py</span> in <span class=\"ansi-cyan-fg\">f_with_exception_handling</span><span class=\"ansi-blue-fg\">(*args, **kwargs)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    387</span>                     exc<span class=\"ansi-blue-fg\">.</span>__context__ <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    388</span>                     exc<span class=\"ansi-blue-fg\">.</span>__cause__ <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-fg\">--&gt; 389</span><span class=\"ansi-red-fg\">                     </span><span class=\"ansi-green-fg\">raise</span> exc\n<span class=\"ansi-green-intense-fg ansi-bold\">    390</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    391</span>             <span class=\"ansi-green-fg\">return</span> f_with_exception_handling\n\n<span class=\"ansi-red-fg\">ExecutionError</span>: An error occurred while calling o490.mount.\n: java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/raw; nested exception is: \n\tjava.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/raw\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:68)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.createOrUpdateMount(DBUtilsCore.scala:756)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:776)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/raw\n\tat scala.Predef$.require(Predef.scala:281)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$insertMount$1(MetadataManager.scala:480)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$modifyAndVerify$1(MetadataManager.scala:845)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.withRetries(MetadataManager.scala:626)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.modifyAndVerify(MetadataManager.scala:834)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.insertMount(MetadataManager.scala:488)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:121)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:103)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:102)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:102)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$3.applyOrElse(DbfsServerBackend.scala:318)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$3.applyOrElse(DbfsServerBackend.scala:277)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$2(ServerBackend.scala:120)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:147)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:147)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:102)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:426)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:520)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:541)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:53)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:49)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:296)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:295)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:24)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:331)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:316)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:24)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:515)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:435)\n\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:24)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:426)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:398)\n\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:24)\n\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:101)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleRPC$2(JettyServer.scala:960)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:960)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:878)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$3(JettyServer.scala:523)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$3$adapted(JettyServer.scala:505)\n\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivity$1(ActivityContextFactory.scala:38)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:53)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:49)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:296)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:295)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:26)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:331)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:316)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionTags(ActivityContextFactory.scala:26)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withActivity(ActivityContextFactory.scala:38)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$2(JettyServer.scala:505)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:53)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:49)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:296)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:295)\n\tat com.databricks.rpc.JettyServer$.withAttributionContext(JettyServer.scala:258)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:331)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:316)\n\tat com.databricks.rpc.JettyServer$.withAttributionTags(JettyServer.scala:258)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:493)\n\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:400)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:707)\n\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:585)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:515)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:539)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:333)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)\n\tat org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:80)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\n\t... 1 more\n</div>","errorSummary":"java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/raw; nested exception is: ","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">ExecutionError</span>                            Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-4499162618750922&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> dbutils.fs.mount(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">      2</span>   source <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">&#34;abfss://raw@aswin.dfs.core.windows.net/&#34;</span><span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span>   mount_point <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">&#34;/mnt/raw&#34;</span><span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span>   extra_configs = configs)\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> \n\n<span class=\"ansi-green-fg\">/databricks/python_shell/dbruntime/dbutils.py</span> in <span class=\"ansi-cyan-fg\">f_with_exception_handling</span><span class=\"ansi-blue-fg\">(*args, **kwargs)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    387</span>                     exc<span class=\"ansi-blue-fg\">.</span>__context__ <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    388</span>                     exc<span class=\"ansi-blue-fg\">.</span>__cause__ <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-fg\">--&gt; 389</span><span class=\"ansi-red-fg\">                     </span><span class=\"ansi-green-fg\">raise</span> exc\n<span class=\"ansi-green-intense-fg ansi-bold\">    390</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    391</span>             <span class=\"ansi-green-fg\">return</span> f_with_exception_handling\n\n<span class=\"ansi-red-fg\">ExecutionError</span>: An error occurred while calling o490.mount.\n: java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/raw; nested exception is: \n\tjava.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/raw\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:68)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.createOrUpdateMount(DBUtilsCore.scala:756)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:776)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/raw\n\tat scala.Predef$.require(Predef.scala:281)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$insertMount$1(MetadataManager.scala:480)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$modifyAndVerify$1(MetadataManager.scala:845)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.withRetries(MetadataManager.scala:626)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.modifyAndVerify(MetadataManager.scala:834)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.insertMount(MetadataManager.scala:488)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:121)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:103)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:102)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:102)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$3.applyOrElse(DbfsServerBackend.scala:318)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$3.applyOrElse(DbfsServerBackend.scala:277)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$2(ServerBackend.scala:120)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:147)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:147)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:102)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:426)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:520)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:541)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:53)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:49)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:296)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:295)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:24)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:331)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:316)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:24)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:515)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:435)\n\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:24)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:426)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:398)\n\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:24)\n\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:101)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleRPC$2(JettyServer.scala:960)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:960)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:878)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$3(JettyServer.scala:523)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$3$adapted(JettyServer.scala:505)\n\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivity$1(ActivityContextFactory.scala:38)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:53)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:49)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:296)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:295)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:26)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:331)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:316)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionTags(ActivityContextFactory.scala:26)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withActivity(ActivityContextFactory.scala:38)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$2(JettyServer.scala:505)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:53)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:49)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:296)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:295)\n\tat com.databricks.rpc.JettyServer$.withAttributionContext(JettyServer.scala:258)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:331)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:316)\n\tat com.databricks.rpc.JettyServer$.withAttributionTags(JettyServer.scala:258)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:493)\n\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:400)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:707)\n\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:585)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:515)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:539)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:333)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)\n\tat org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:80)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\n\t... 1 more\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#Main Program"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5ba2a2ae-893a-4530-9b47-17305d375d29"}}},{"cell_type":"code","source":["#Main Program\n\n#Creation of Data Frames\n\n\nInd_Df=create_DF_IND() \n\nUsa_Df=create_DF_USA()\n\nAus_df=create_DF_AUS()\n\n\n\n\n#Merging into single source of truth\n\n\nMerged_DF=IND_USA_Df=Ind_Df.union(Usa_Df) #aggregating ind and usa data\nMerged_DF=IND_USA_AUS_Df=IND_USA_Df.union(Aus_df)  #agregagting the dataframe containg ind and usa data with aus data to generate dataframe containing aggregated data\n\n\n\n#Creating view on top of merged dataframe containing agggregated data \n\nMerged_DF.createOrReplaceTempView(\"temp_table\") \n\n\n\n#This view will be helpful for querying data which we will be using to build the final report tables in next steps.The final report table will serve as a input to Power BI\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0ec7d635-0edc-460f-a854-511c3dc29474"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Data frame  created and Formatted Succesfully for Ind Dataset\nData frame  created and Formatted Succesfully for USA dataset\nData frame  created and Formatted Succesfully for Aus dataset\n+-------+---------+-------------+---------------+---------------+------------+\n|Country|     Name|Date of Birth|VaccinationType|VaccinationDate|Free or Paid|\n+-------+---------+-------------+---------------+---------------+------------+\n|    IND|    Vikas|   1998-12-01|            XYZ|     2022-01-01|           F|\n|    IND|    Rahul|   1982-08-13|            ABC|     2022-03-05|           P|\n|    IND|   Sameer|   1952-08-13|            ABC|     2022-02-20|           F|\n|    USA|      Sam|         null|            EFG|     2022-06-15|           P|\n|    USA|     John|         null|            XYZ|     2022-01-05|           P|\n|    USA|     Mike|         null|            ABC|     2021-12-28|           P|\n|    AUS|     Mike|         null|            LMN|       22-05-11|           P|\n|    AUS|Jonnathan|     97-12-13|            XYZ|           null|           P|\n|    AUS| Cristina|     98-03-12|            ABC|       22-03-12|           P|\n+-------+---------+-------------+---------------+---------------+------------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Data frame  created and Formatted Succesfully for Ind Dataset\nData frame  created and Formatted Succesfully for USA dataset\nData frame  created and Formatted Succesfully for Aus dataset\n+-------+---------+-------------+---------------+---------------+------------+\nCountry|     Name|Date of Birth|VaccinationType|VaccinationDate|Free or Paid|\n+-------+---------+-------------+---------------+---------------+------------+\n    IND|    Vikas|   1998-12-01|            XYZ|     2022-01-01|           F|\n    IND|    Rahul|   1982-08-13|            ABC|     2022-03-05|           P|\n    IND|   Sameer|   1952-08-13|            ABC|     2022-02-20|           F|\n    USA|      Sam|         null|            EFG|     2022-06-15|           P|\n    USA|     John|         null|            XYZ|     2022-01-05|           P|\n    USA|     Mike|         null|            ABC|     2021-12-28|           P|\n    AUS|     Mike|         null|            LMN|       22-05-11|           P|\n    AUS|Jonnathan|     97-12-13|            XYZ|           null|           P|\n    AUS| Cristina|     98-03-12|            ABC|       22-03-12|           P|\n+-------+---------+-------------+---------------+---------------+------------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#Generation Output to be used by Power BI"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3dacf0ca-4d63-4868-99e3-76227dbde7dd"}}},{"cell_type":"markdown","source":["#vaccination count by country and vaccination Type"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3a23dd38-1fbf-436f-9822-392c8fecc2b8"}}},{"cell_type":"code","source":["%sql\n\n--We use the query to build the table that has the %  vaccination contribution in each country assming its total population 1000000 to be and this table can be input to power BI\nCreate table Vaccination_Type_by_Country as (\nselect Country,vaccinationType,count(vaccinationType)  number_of_vaccination \nfrom temp_table \ngroup by VaccinationType,Country)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d2059808-c72d-4fe2-b7e9-11b7f666cb3c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"num_affected_rows","type":"\"long\"","metadata":"{}"},{"name":"num_inserted_rows","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th><th>num_inserted_rows</th></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#% Vaccination in each country"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"131b7795-57ff-40d3-83dc-a3f3c94475ce"}}},{"cell_type":"code","source":["%sql\n\n--We use the query to buld the table that has the %  vaccination contribution in each country assming its total population 1000000 to be and this table can be input to power BI\ndrop table if exists Percent_Vaccination ;\ncreate table Percent_Vaccination as \n(select country ,count(country)/1000000*100 percentage_vaccinated \nfrom temp_table \ngroup by country)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"47448cc4-f3bd-4855-bd7a-a015b258f8d1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"num_affected_rows","type":"\"long\"","metadata":"{}"},{"name":"num_inserted_rows","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th><th>num_inserted_rows</th></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#% vaccination contribution by country"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"196b49b1-cb22-4c7e-b312-a1c948150831"}}},{"cell_type":"code","source":["%sql\n--We use the query to buld the table that has the %  vaccination contribution by country and this table can be input to power BI\nCreate table  Percentage_Vaccine_Contribution  as (\nSELECT country,count(country) as count_country,\ncount(country) * 100.0 / (select count(*) from temp_table) as Percent_contribution\nFROM temp_table\ngroup by country)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5c3fcabf-14fc-46f2-adec-f5011345697d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"num_affected_rows","type":"\"long\"","metadata":"{}"},{"name":"num_inserted_rows","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th><th>num_inserted_rows</th></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#Functions to create dataframe for data of each country.Each dataframe is implemented seprately to manage wide range of data format inconsistency that is already present or may arise in the Future"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a81becb4-b388-4edb-9b2b-f2dc409177cf"}}},{"cell_type":"markdown","source":["#Creating DataFrame for India Data Set"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a044d161-e7ca-4f8b-9c49-20eeb2b9a2bb"}}},{"cell_type":"code","source":["from pyspark.sql.functions import lit\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.functions import to_date\n\n\ndef create_DF_IND():\n    Ind_Df = spark.read.format(\"csv\") \\\n    .option(\"inferSchema\", \"true\") \\\n    .option(\"header\", \"true\") \\\n    .option(\"sep\", \",\") \\\n    .load(\"dbfs:/mnt/raw/IND.csv\")\n    if 'DOB' in Ind_Df.columns:\n        Ind_Df = Ind_Df.withColumnRenamed('DOB', 'Date of Birth')\n    #Formatting_Date\n\n    Ind_Df = Ind_Df.withColumn(\"Date of Birth\",to_date(col(\"Date of Birth\"),\"yyyy-MM-dd\"))\n    Ind_Df = Ind_Df.withColumn(\"VaccinationDate\",to_date(col(\"VaccinationDate\"),\"yyyy-MM-dd\"))\n        \n    #Adding missing columns\n    \n    Ind_Df = Ind_Df.withColumn('Country', lit(\"IND\").cast(\"String\"))\n     #Creating final df with proper column order\n        \n    Ind_Df = Ind_Df[\"Country\",\"Name\",\"Date of Birth\",\"VaccinationType\",\"VaccinationDate\",\"Free or Paid\"]\n    \n    message=\"Data frame  created and Formatted Succesfully for Ind Dataset\"\n    print(message)\n    \n    return Ind_Df\n\n\n    \n\n    "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5d69477e-f0f5-4758-a107-6c3b803419bd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Creating DataFrame for USA Dataset"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bd470cb3-3317-4507-989b-14dac5fd4b53"}}},{"cell_type":"code","source":["# Read the csv files with first line as header, comma (,) as separator, and detect schema from the file\nimport pyspark.sql.functions as F\n\ndef create_DF_USA():\n    Usa_Df = spark.read.format(\"csv\") \\\n    .option(\"inferSchema\", \"false\") \\\n    .option(\"header\", \"true\") \\\n    .option(\"sep\", \",\") \\\n    .load(\"/mnt/raw/USA.csv\")\n    \n \n   \n    #Adding Missing Columns\n    \n    if 'Date of Birth' not in Usa_Df.columns:\n        Usa_Df = Usa_Df.withColumn('Date of Birth', lit(None).cast(\"String\"))\n    if 'Free or Paid' not in Usa_Df.columns:\n        Usa_Df = Usa_Df.withColumn('Free or Paid', lit(\"P\").cast(\"String\"))\n        \n     #Adding column with country value as USA\n        Usa_Df = Usa_Df.withColumn('Country', lit(\"USA\").cast(\"String\"))\n        \n     #Stripping of leading 0s\n        Usa_Df = Usa_Df.withColumn('Date of Birth', F.regexp_replace('Date of Birth', r'^[0]*', ''))\n     #Formatting_Dates\n    \n        Usa_Df = Usa_Df.withColumn(\"VaccinationDate\",date_formatter(\"VaccinationDate\"))\n        \n        \n     #Creating final df with proper column order\n    \n    Usa_Df = Usa_Df[\"Country\",\"Name\",\"Date of Birth\",\"VaccinationType\",\"VaccinationDate\",\"Free or Paid\"]\n    \n    message=\"Data frame  created and Formatted Succesfully for USA dataset\"\n    print(message)\n    return Usa_Df\n    "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bf886f49-2171-46a4-a81b-98e50b719590"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Creating Data Frame for Australia Dataset"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"422afb7f-49f7-4f1f-8135-1c6354e1fa3c"}}},{"cell_type":"code","source":["from pyspark.sql.functions import lit\nfrom pyspark.sql.functions import *\nimport pyspark.sql.functions as F\n\n\ndef create_DF_AUS():\n    \n    Aus_df = spark.read.format(\"com.crealytics.spark.excel\") \\\n    .option(\"header\", \"true\") \\\n    .option(\"inferSchema\", \"false\") \\\n    .load(\"/mnt/raw/AUS.xlsx\")\n    spark.conf.set(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\") \n     #Renaming Columns to match schema\n    if 'Unique ID' in Aus_df.columns:\n        Aus_df = Aus_df.withColumnRenamed('Unique ID', 'ID')\n    if 'Patient Name' in Aus_df.columns:\n        Aus_df = Aus_df.withColumnRenamed('Patient Name', 'Name')\n    if 'Vaccine Type' in Aus_df.columns:\n         Aus_df = Aus_df.withColumnRenamed('Vaccine Type', 'VaccinationType')\n    if 'Date of Vaccination' in Aus_df.columns:\n          Aus_df = Aus_df.withColumnRenamed('Date of Vaccination', 'VaccinationDate')\n            \n    #Adding missing columns\n    \n    if 'Free or Paid' not in Aus_df.columns:\n          Aus_df = Aus_df.withColumn('Free or Paid', lit(\"P\").cast(\"String\"))\n          Aus_df = Aus_df.withColumn(\"VaccinationDate\",date_formatter(\"VaccinationDate\"))\n        \n    #Formatting Date\n          Aus_df = Aus_df.withColumn(\"Date of Birth\",date_formatter(\"Date of Birth\"))\n          Aus_df = Aus_df.withColumn('Date of Birth', F.regexp_replace('Date of Birth', r'^[0]*', ''))\n          Aus_df = Aus_df.withColumn('VaccinationDate', F.regexp_replace('VaccinationDate', r'^[0]*', ''))\n        \n    #Adding country field as AUS to indicate it came databse for Australia\n          Aus_df = Aus_df.withColumn('Country', lit(\"AUS\").cast(\"String\"))\n          Aus_df = Aus_df[\"Country\",\"Name\",\"Date of Birth\",\"VaccinationType\",\"VaccinationDate\",\"Free or Paid\"]\n    \n\n    message=\"Data frame  created and Formatted Succesfully for Aus dataset\"\n    print(message)\n    return Aus_df\n    "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"340307ce-4d7d-4c9e-b577-fd9285c9709d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#Date Formatter Function"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3b4b501a-0581-453d-9f8d-ed555039b19a"}}},{"cell_type":"code","source":["#Date_Formatter \n#Known Bugs with some format\n#Not fully working\n\n\nfrom pyspark.sql.functions import coalesce, to_date , to_timestamp\n\ndef date_formatter(col, formats=(\"MM/dd/yyyy\",\"yyyy-MM-dd\",\"MM-dd-yy\",\"MMddyyyy\")):\n   \n    return coalesce(*[to_date(col, f) for f in formats])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"64183147-c968-46fa-8669-525b94a167e8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Vaccination_Report","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":4499162618750917}},"nbformat":4,"nbformat_minor":0}
